
# Task 05 â€“ Descriptive Stats
**Comparing Python ground-truth vs. ChatGPT reasoning on Formula 1 data**

![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue)
![pandas 2.x](https://img.shields.io/badge/pandas-2.x-green)
![LLM â€“ ChatGPT](https://img.shields.io/badge/LLM-ChatGPT-ff69b4)

---

## 1Â Â Overview
This repo fulfils **Research Task 05** (SummerÂ 2025) for Prof. JeffÂ Strome at Syracuse iSchool. We take a *small* slice of the public **Ergast FormulaÂ 1** database and

1. **Answer 10 intermediate/hard questions in Python** (groundâ€‘truth).  
2. **Ask ChatGPT the same questions** using the sliced tables as context.  
3. Log where the LLM matches âœ“Â or diverges âœ—Â from Python results.

---

## 2Â Â Repo layout
```text
Task_05_Descriptive_Stats/
â”œâ”€ Data/                       # raw CSVs (gitâ€‘ignored)
â”‚   â”œâ”€ drivers.csv
â”‚   â”œâ”€ races.csv
â”‚   â””â”€ â€¦                       # lap_times, pit_stops, â€¦
â”‚
â”œâ”€ slice_f1.py                 # creates tiny 2024 slices (<1Â kB)
â”œâ”€ analysis.py                 # helpers Q1â€“Q10 (import as f1)
â”œâ”€ notebooks/
â”‚   â””â”€ Task_05_Descriptive_Stats.ipynb
â”‚
â”œâ”€ prompts/                    # ChatGPT prompt templates
â”‚   â”œâ”€ system.txt
â”‚   â””â”€ q1-q10_examples.txt
â”‚
â”œâ”€ failures.md                 # ChatGPT mismatches go here
â”œâ”€ requirements.txt
â””â”€ README.md                   # â† youâ€™re reading it
```

---

## 3Â Â Quickâ€‘start
```bash
# Clone the repo
git clone https://github.com/<user>/Task_05_Descriptive_Stats.git
cd Task_05_Descriptive_Stats

# ğŸ Create & activate a fresh virtualâ€‘env
python -m venv .venv            # make env
source .venv/bin/activate       # activate (Linux/macOS)
# .venv\Scripts\activate      # activate (Windows PowerShell)

# ğŸ“¦ Install Python dependencies
pip install -r requirements.txt

# âœ‚ï¸ Slice the bulky Ergast dump into two tiny CSVs for 2024
python slice_f1.py              # outputs standings_2024.csv, pitstops_2024_final_gp.csv

# ğŸ“ Launch the Jupyter notebook
jupyter lab notebooks/Task_05_Descriptive_Stats.ipynb
```

> **Headsâ€‘up:** raw Ergast CSVs live in **Data/** and are **.gitignored**.  
> The slice script keeps derivative tables under 1Â kB so they *can* live in Git.

---

## 4Â Â Python groundâ€‘truth answers
| Q | Metric | 2024 (unless noted) |
|---|--------------------------------|---------------------------|
| 1 | Avg. gridÂ â†’Â flag gain | **Sergio PÃ©rez**Â +3.75 |
| 2 | Pitâ€‘stop ÏƒÂ (<60â€¯s) | **McLaren**Â 3.57â€¯s |
| 3 | Highest overtakeâ€‘rate GP | **LasÂ Vegas**Â 2.54â€¯ov/lap |
| 4 | Biggest singleâ€‘lap gain | **D.Â Ricciardo**Â âˆ’2â€¯990â€¯ms (CANÂ Lapâ€¯12) |
| 5 | Largest points swing | **M.Â Verstappen**Â +33 (JPNÂ â†’Â CHN) |
| 6 | PointsÂ / pitâ€‘sec efficiency | **Ferrari**Â 8.20 |
| 7 | Pitâ€‘stop modelÂ RÂ² | **0.019** |
| 8 | Fastâ€‘lap specialist (â€™20â€‘â€™24) | **Verstappen**Â 26.9â€¯% |
| 9 | Teamâ€‘mate quali gap | **RedÂ Bull**Â 6.58 |
|10 | YoY pts/â€‹race gain (â€™23â†’â€™24) | **McLaren**Â +14.0Â (+102â€¯%) |

All generated by `analysis.py`; rerun any helper with  
`import analysis as f1; f1.<function>(â€¦)`.

---

## 5Â Â ChatGPT comparison log
After running each prompt in **prompts/**, paste ChatGPTâ€™s reply below, mark **âœ“** if it matches the Python groundâ€‘truth or **âœ—** if it diverges.  
Put detailed explanations in **failures.md**.

| Q# | What was asked | My ChatGPT answer | Python notebook answer | Accuracy |
|----|----------------|-------------------|-----------------------|----------|
| 1 | Avg. gridâ€‘positions gained (2024) | Drvâ€¯860Â â€“Â +2.33 | Drvâ€¯815Â (S.Â PÃ©rez)Â â€“Â +3.75 | âœ— |
| 2 | Most consistent pitâ€‘stop team (ÏƒÂ <Â 5â€¯s) | unknown | McLarenÂ (Ïƒâ€¯=â€¯3.565â€¯s) | âœ— |
| 3 | Highest overtakeâ€‘rate circuit (2024) | unknown | LasÂ VegasÂ GPÂ â€“Â 2.54Â ov/lap | âœ— |
| 4 | Biggest singleâ€‘lap improvement | ZhouÂ Guanyu, lapâ€¯2Â MON,Â âˆ’2â€¯414â€¯122â€¯ms | DanielÂ Ricciardo, lapâ€¯12Â CAN,Â âˆ’2â€¯990â€¯ms | âœ— |
| 5 | Largest points swing between races | Drvâ€¯830Â &â€¯846,Â +26â€¯pts | MaxÂ Verstappen,Â +33â€¯pts | âœ— |
| 6 | PointsÂ / pitâ€‘stopâ€‘sec efficiency | unknown | FerrariÂ â€“Â 8.20â€¯pts/s | âœ— |
| 7 | Linear model on pitâ€‘stop time | RÂ²Â =Â 0.098; topÂ coeffsÂ {210,â€¯3,â€¯117} | RÂ²Â =Â 0.0192; coeffs differ | âœ— |
| 8 | Fastâ€‘lapâ€‘rate leader (2020â€‘24) | Drvâ€¯830,Â 0.243 | Drvâ€¯830,Â 0.269 | â–³ |
| 9 | Teamâ€‘mate qualifying duel (2024) | RBRÂ â€“Â 830Â vsÂ 815Â =Â 6.42 | RBRÂ â€“Â 6.58 | âœ“ |
| 10 | YoY ptsâ€‘perâ€‘race gain (â€™23â†’â€™24) | McLarenÂ +14.02Â (+102â€¯%) | McLarenÂ +14.02Â (+102â€¯%) | âœ“ |

---

## 6Â Â How `analysis.py` works
* **Pure Pandas**â€”no API calls.  
* Uses `results.csv` for reliable driverÂ â†’Â constructor mapping.  
* Heavy tables are filtered to one season or one GP.  
* Each helper returns a tidy DataFrameÂ / Series ready for notebook display or TSV export to ChatGPT.

---

## 7Â Â Dataset licence
Data Â© **[Ergast DeveloperÂ DB](https://ergast.com/mrd/)** â€” CCÂ BYÂ 4.0.  
Raw CSVs are *not* committed; only tiny derivative slices remain in Git.

---

## 8Â Â Requirements
* PythonÂ 3.10+  
* pandas, numpy, scikitâ€‘learn, matplotlib, jupyter  
  (see `requirements.txt` for exact versions)

---

## 9Â Â Credits
**RijulÂ Chaturvedi** â€” M.S. Applied Data Science, Syracuse University  
Advisor: **Prof.Â JeffÂ Strome** (`jrstrome@syr.edu`)



---------------------


# Part II â€” Advanced Descriptive Analysis (F1)

This addendum extends **Task 05 â€“ Descriptive Statistics** with deeper metrics, robust preprocessing, and publication-ready visuals. It is designed to be pasted into your repository README. All analyses run locally on the CSVs in `Data/` and export tidy tables (CSV) and figures (PNG) suitable for reports.

---

## ğŸ“š Contents
- [Whatâ€™s new](#whats-new)
- [Quick start](#quick-start)
- [Analyses](#analyses)
  - [1) Pit-Stop Consistency Deep-Dive](#1-pit-stop-consistency-deep-dive)
  - [2) Constructor Efficiency â€” Points per Pit-Stop Second (2024)](#2-constructor-efficiency--points-per-pit-stop-second-2024)
  - [3) Team-mate Qualifying Duel â€” Average Grid Advantage (2024)](#3-team-mate-qualifying-duel--average-grid-advantage-2024)
  - [4) Reliability â€” DNF Rate by Constructor (2024)](#4-reliability--dnf-rate-by-constructor-2024)
  - [5) Fastest-Lap Specialist â€” Rate by Driver (2020â€“2024)](#5-fastest-lap-specialist--rate-by-driver-20202024)
- [Outputs](#outputs)
- [Figures](#figures)
- [Methods & assumptions](#methods--assumptions)
- [Sensitivity & robustness](#sensitivity--robustness)
- [Troubleshooting](#troubleshooting)
- [Reproducibility checklist](#reproducibility-checklist)
- [Repo structure (suggested)](#repo-structure-suggested)
- [Changelog](#changelog)
- [Next steps](#next-steps)

---

## Whatâ€™s new

- **New metrics**: pit-stop **consistency**, **efficiency (pts/sec)**, **qualifying dominance**, **reliability (DNF rate)**, and **fastest-lap rate**.
- **Robust parsing**: flexible handling of driver/constructor names (`forename` vs `forenames`, `name` vs `constructorRef`), and pit stop **`duration`** formats (`ss.sss`, `m:ss.sss`, `h:mm:ss.sss`).
- **Clean exports**: CSV tables + single-plot, Matplotlib-only PNGs; bootstrap CIs for pit-stop means.
- **Drop-in scripts**: minimal-dependency CLI (`simple_part2.py`) and notebook cells for each analysis.

---

## Quick start

### Option A â€” Notebook
Open your Part II notebook and run cells in order. Each analysis cell saves a CSV to the repo root (or `outputs/`) and displays a figure.

### Option B â€” Script (minimal deps: `pandas`, `numpy`, `matplotlib`)
```bash
python simple_part2.py \
  --input Data/pit_stops.csv \
  --outdir figures_part2 \
  --target milliseconds \
  --group driverId

> *â€œTwo sources are better than oneâ€”especially when one is an LLM.â€* ğŸš€
