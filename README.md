
# Task 05 – Descriptive Stats
**Comparing Python ground-truth vs. ChatGPT reasoning on Formula 1 data**

![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue)
![pandas 2.x](https://img.shields.io/badge/pandas-2.x-green)
![LLM – ChatGPT](https://img.shields.io/badge/LLM-ChatGPT-ff69b4)

---

## 1  Overview
This repo fulfils **Research Task 05** (Summer 2025) for Prof. Jeff Strome at Syracuse iSchool. We take a *small* slice of the public **Ergast Formula 1** database and

1. **Answer 10 intermediate/hard questions in Python** (ground‑truth).  
2. **Ask ChatGPT the same questions** using the sliced tables as context.  
3. Log where the LLM matches ✓ or diverges ✗ from Python results.

---

## 2  Repo layout
```text
Task_05_Descriptive_Stats/
├─ Data/                       # raw CSVs (git‑ignored)
│   ├─ drivers.csv
│   ├─ races.csv
│   └─ …                       # lap_times, pit_stops, …
│
├─ slice_f1.py                 # creates tiny 2024 slices (<1 kB)
├─ analysis.py                 # helpers Q1–Q10 (import as f1)
├─ notebooks/
│   └─ Task_05_Descriptive_Stats.ipynb
│
├─ prompts/                    # ChatGPT prompt templates
│   ├─ system.txt
│   └─ q1-q10_examples.txt
│
├─ failures.md                 # ChatGPT mismatches go here
├─ requirements.txt
└─ README.md                   # ← you’re reading it
```

---

## 3  Quick‑start
```bash
# Clone the repo
git clone https://github.com/<user>/Task_05_Descriptive_Stats.git
cd Task_05_Descriptive_Stats

# 🐍 Create & activate a fresh virtual‑env
python -m venv .venv            # make env
source .venv/bin/activate       # activate (Linux/macOS)
# .venv\Scripts\activate      # activate (Windows PowerShell)

# 📦 Install Python dependencies
pip install -r requirements.txt

# ✂️ Slice the bulky Ergast dump into two tiny CSVs for 2024
python slice_f1.py              # outputs standings_2024.csv, pitstops_2024_final_gp.csv

# 📝 Launch the Jupyter notebook
jupyter lab notebooks/Task_05_Descriptive_Stats.ipynb
```

> **Heads‑up:** raw Ergast CSVs live in **Data/** and are **.gitignored**.  
> The slice script keeps derivative tables under 1 kB so they *can* live in Git.

---

## 4  Python ground‑truth answers
| Q | Metric | 2024 (unless noted) |
|---|--------------------------------|---------------------------|
| 1 | Avg. grid → flag gain | **Sergio Pérez** +3.75 |
| 2 | Pit‑stop σ (<60 s) | **McLaren** 3.57 s |
| 3 | Highest overtake‑rate GP | **Las Vegas** 2.54 ov/lap |
| 4 | Biggest single‑lap gain | **D. Ricciardo** −2 990 ms (CAN Lap 12) |
| 5 | Largest points swing | **M. Verstappen** +33 (JPN → CHN) |
| 6 | Points / pit‑sec efficiency | **Ferrari** 8.20 |
| 7 | Pit‑stop model R² | **0.019** |
| 8 | Fast‑lap specialist (’20‑’24) | **Verstappen** 26.9 % |
| 9 | Team‑mate quali gap | **Red Bull** 6.58 |
|10 | YoY pts/​race gain (’23→’24) | **McLaren** +14.0 (+102 %) |

All generated by `analysis.py`; rerun any helper with  
`import analysis as f1; f1.<function>(…)`.

---

## 5  ChatGPT comparison log
After running each prompt in **prompts/**, paste ChatGPT’s reply below, mark **✓** if it matches the Python ground‑truth or **✗** if it diverges.  
Put detailed explanations in **failures.md**.

| Q# | What was asked | My ChatGPT answer | Python notebook answer | Accuracy |
|----|----------------|-------------------|-----------------------|----------|
| 1 | Avg. grid‑positions gained (2024) | Drv 860 – +2.33 | Drv 815 (S. Pérez) – +3.75 | ✗ |
| 2 | Most consistent pit‑stop team (σ < 5 s) | unknown | McLaren (σ = 3.565 s) | ✗ |
| 3 | Highest overtake‑rate circuit (2024) | unknown | Las Vegas GP – 2.54 ov/lap | ✗ |
| 4 | Biggest single‑lap improvement | Zhou Guanyu, lap 2 MON, −2 414 122 ms | Daniel Ricciardo, lap 12 CAN, −2 990 ms | ✗ |
| 5 | Largest points swing between races | Drv 830 & 846, +26 pts | Max Verstappen, +33 pts | ✗ |
| 6 | Points / pit‑stop‑sec efficiency | unknown | Ferrari – 8.20 pts/s | ✗ |
| 7 | Linear model on pit‑stop time | R² = 0.098; top coeffs {210, 3, 117} | R² = 0.0192; coeffs differ | ✗ |
| 8 | Fast‑lap‑rate leader (2020‑24) | Drv 830, 0.243 | Drv 830, 0.269 | △ |
| 9 | Team‑mate qualifying duel (2024) | RBR – 830 vs 815 = 6.42 | RBR – 6.58 | ✓ |
| 10 | YoY pts‑per‑race gain (’23→’24) | McLaren +14.02 (+102 %) | McLaren +14.02 (+102 %) | ✓ |

---

## 6  How `analysis.py` works
* **Pure Pandas**—no API calls.  
* Uses `results.csv` for reliable driver → constructor mapping.  
* Heavy tables are filtered to one season or one GP.  
* Each helper returns a tidy DataFrame / Series ready for notebook display or TSV export to ChatGPT.

---

## 7  Dataset licence
Data © **[Ergast Developer DB](https://ergast.com/mrd/)** — CC BY 4.0.  
Raw CSVs are *not* committed; only tiny derivative slices remain in Git.

---

## 8  Requirements
* Python 3.10+  
* pandas, numpy, scikit‑learn, matplotlib, jupyter  
  (see `requirements.txt` for exact versions)

---

## 9  Credits
**Rijul Chaturvedi** — M.S. Applied Data Science, Syracuse University  
Advisor: **Prof. Jeff Strome** (`jrstrome@syr.edu`)



---------------------


# Part II — Advanced Descriptive Analysis (F1)

This addendum extends **Task 05 – Descriptive Statistics** with deeper metrics, robust preprocessing, and publication-ready visuals. It is designed to be pasted into your repository README. All analyses run locally on the CSVs in `Data/` and export tidy tables (CSV) and figures (PNG) suitable for reports.

---

## 📚 Contents
- [What’s new](#whats-new)
- [Quick start](#quick-start)
- [Analyses](#analyses)
  - [1) Pit-Stop Consistency Deep-Dive](#1-pit-stop-consistency-deep-dive)
  - [2) Constructor Efficiency — Points per Pit-Stop Second (2024)](#2-constructor-efficiency--points-per-pit-stop-second-2024)
  - [3) Team-mate Qualifying Duel — Average Grid Advantage (2024)](#3-team-mate-qualifying-duel--average-grid-advantage-2024)
  - [4) Reliability — DNF Rate by Constructor (2024)](#4-reliability--dnf-rate-by-constructor-2024)
  - [5) Fastest-Lap Specialist — Rate by Driver (2020–2024)](#5-fastest-lap-specialist--rate-by-driver-20202024)
- [Outputs](#outputs)
- [Figures](#figures)
- [Methods & assumptions](#methods--assumptions)
- [Sensitivity & robustness](#sensitivity--robustness)
- [Troubleshooting](#troubleshooting)
- [Reproducibility checklist](#reproducibility-checklist)
- [Repo structure (suggested)](#repo-structure-suggested)
- [Changelog](#changelog)
- [Next steps](#next-steps)

---

## What’s new

- **New metrics**: pit-stop **consistency**, **efficiency (pts/sec)**, **qualifying dominance**, **reliability (DNF rate)**, and **fastest-lap rate**.
- **Robust parsing**: flexible handling of driver/constructor names (`forename` vs `forenames`, `name` vs `constructorRef`), and pit stop **`duration`** formats (`ss.sss`, `m:ss.sss`, `h:mm:ss.sss`).
- **Clean exports**: CSV tables + single-plot, Matplotlib-only PNGs; bootstrap CIs for pit-stop means.
- **Drop-in scripts**: minimal-dependency CLI (`simple_part2.py`) and notebook cells for each analysis.

---

## Quick start

### Option A — Notebook
Open your Part II notebook and run cells in order. Each analysis cell saves a CSV to the repo root (or `outputs/`) and displays a figure.

### Option B — Script (minimal deps: `pandas`, `numpy`, `matplotlib`)
```bash
python simple_part2.py \
  --input Data/pit_stops.csv \
  --outdir figures_part2 \
  --target milliseconds \
  --group driverId

> *“Two sources are better than one—especially when one is an LLM.”* 🚀
