# Task 05 â€“ Descriptive Stats
**Comparing Python ground-truth vs. ChatGPT reasoning on Formula 1 data**

![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue)
![pandas 2.x](https://img.shields.io/badge/pandas-2.x-green)
![LLM â€“ ChatGPT](https://img.shields.io/badge/LLM-ChatGPT-ff69b4)

---

## 1  Overview
This repo fulfils **Research Task 05** (Summer 2025) for Prof. Jeff Strome at Syracuse iSchool.  We take a *small* slice of the public **Ergast Formula 1** database and

1. **Answer 10 intermediate/hard questions in Python** (ground-truth).
2. **Ask ChatGPT the same questions** using the sliced tables as context.
3. Log where the LLM matches âœ“ or diverges âœ— from Python results.

---

## 2  Repo layout
```
Task_05_Descriptive_Stats/
â”œâ”€ Data/                       # raw CSVs (git-ignored)
â”‚   â”œâ”€ drivers.csv
â”‚   â”œâ”€ races.csv
â”‚   â””â”€ â€¦                       # lap_times, pit_stops, â€¦
â”‚
â”œâ”€ slice_f1.py                 # creates tiny 2024 slices (<1 kB)
â”œâ”€ analysis.py                 # helpers Q1â€“Q10 (import as f1)
â”œâ”€ notebooks/
â”‚   â””â”€ Task_05_Descriptive_Stats.ipynb
â”‚
â”œâ”€ prompts/                    # ChatGPT prompt templates
â”‚   â”œâ”€ system.txt
â”‚   â””â”€ q1-q10_examples.txt
â”‚
â”œâ”€ failures.md                 # ChatGPT mismatches go here
â”œâ”€ requirements.txt
â””â”€ README.md                   # â† youâ€™re reading it
```

---

## 3  Quick-start
```bash
# Clone the repo
git clone https://github.com/<user>/Task_05_Descriptive_Stats.git
cd Task_05_Descriptive_Stats

# ğŸ Create & activate a fresh virtual-env
python -m venv .venv            # make env
source .venv/bin/activate       # activate (Linux/macOS)
# .venv\Scripts\activate        # activate (Windows PowerShell)

# ğŸ“¦ Install Python dependencies
pip install -r requirements.txt

# âœ‚ï¸  Slice the bulky Ergast dump into two tiny CSVs for 2024
python slice_f1.py              # outputs standings_2024.csv, pitstops_2024_final_gp.csv

# ğŸ“ Launch the Jupyter notebook
jupyter lab notebooks/Task_05_Descriptive_Stats.ipynb
```

> **Heads-up:** raw Ergast CSVs live in **Data/** and are **.gitignored**.  The slice script keeps derivative tables under 1 kB so they *can* live in Git.

---

## 4  Python ground-truth answers
| Q | Metric | 2024 (unless noted) |
|---|--------|---------------------|
| 1 | Avg. grid â†’ flag gain | **Sergio PÃ©rez** +3.75 |
| 2 | Pit-stop Ïƒ (<60 s) | **McLaren** 3.57 s |
| 3 | Highest overtake-rate GP | **Las Vegas** 2.54 ov/â€‹lap |
| 4 | Biggest single-lap gain | **D. Ricciardo** âˆ’2 990 ms (Canada Lap 12) |
| 5 | Largest pts swing | **M. Verstappen** +33 (Japan â†’ China) |
| 6 | Points / pit-sec efficiency | **Ferrari** 8.20 |
| 7 | Pit-stop model RÂ² | **0.019** |
| 8 | Fast-lap specialist (â€™20-â€™24) | **Verstappen** 26.9 % |
| 9 | Team-mate quali gap | **Red Bull** 6.58 |
|10 | YoY pts/â€‹race gain (â€™23â†’â€™24) | **McLaren** +14.0 (+102 %) |

All generated by `analysis.py`; rerun any helper with `import analysis as f1; f1.<function>(...)`.

---

## 5  ChatGPT comparison log
After running each prompt in **prompts/**, paste ChatGPTâ€™s reply here and tick match status. Put detailed explanations in **failures.md**.

| Q | ChatGPT answer | Match âœ“ / âœ— | Brief notes |
|---|----------------|-------------|-------------|
| 1 | Driver 860 (+2.33) | âœ— | mis-mapped IDs |
| 2 | McLaren Ïƒ 3.57 | âœ“ | â€” |
| â€¦ | â€¦ | â€¦ | â€¦ |

---

## 6  How `analysis.py` works
* **Pure Pandas**â€”no API calls.  
* Uses `results.csv` for reliable driver â†’ constructor mapping.  
* Heavy tables are filtered to one season or one GP.  
* Each helper returns a tidy DataFrame / Series ready for notebook display or TSV export to ChatGPT.

---

## 7  Dataset licence
Data Â© [Ergast Developer DB](https://ergast.com/mrd/) â€” CC BY 4.0.  
Raw CSVs are *not* committed; only tiny derivative slices remain in Git.

---

## 8  Requirements
* Python 3.10+  
* pandas, numpy, scikit-learn, matplotlib, jupyter (see `requirements.txt`)

---

## 9  Timeline checkpoints
| Date | Deliverable |
|------|-------------|
| **31 Jul 2025** | Qualtrics #1 â€” dataset, baseline stats, early LLM runs |
| **15 Aug 2025** | Qualtrics #2 â€” advanced prompts, failure log, visuals |

---

## 10  Credits
**Rijul Chaturvedi** â€” M.S. Applied Data Science, Syracuse University  
Advisor: **Prof. Jeff Strome** (`jrstrome@syr.edu`)

> *â€œTwo sources are better than oneâ€”especially when one is an LLM.â€* ğŸš€
